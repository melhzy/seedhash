{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514fb085",
   "metadata": {},
   "source": [
    "# üöÄ Advanced ML Paradigms\n",
    "\n",
    "## SeedHash Tutorial #3: Semi-Supervised, Reinforcement & Federated Learning\n",
    "\n",
    "**Welcome!** This advanced notebook covers cutting-edge ML paradigms:\n",
    "- Semi-supervised learning with pseudo-labeling\n",
    "- Reinforcement learning with episode tracking\n",
    "- Federated learning with client fairness\n",
    "\n",
    "**Prerequisites**: Complete tutorials #1 and #2 first\n",
    "\n",
    "**Duration**: ~60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Advanced Paradigms\n",
    "2. Semi-Supervised Learning\n",
    "3. Semi-Supervised Metrics\n",
    "4. Reinforcement Learning\n",
    "5. RL Metrics & Tracking\n",
    "6. Federated Learning\n",
    "7. Federated Learning Metrics\n",
    "8. Complete Example: Multi-Paradigm Study\n",
    "9. Best Practices\n",
    "10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8234a",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '../Python')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seedhash import SeedExperimentManager\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bebca4",
   "metadata": {},
   "source": [
    "## 1. Introduction to Advanced Paradigms üåü\n",
    "\n",
    "Modern ML goes beyond traditional supervised/unsupervised learning:\n",
    "\n",
    "### Semi-Supervised Learning\n",
    "- Uses **small amounts of labeled data** + large amounts of unlabeled data\n",
    "- Pseudo-labeling: label confident predictions\n",
    "- Critical for expensive labeling tasks\n",
    "\n",
    "### Reinforcement Learning\n",
    "- Agent learns through **trial and error**\n",
    "- Episode-based training with rewards\n",
    "- Applications: robotics, games, control systems\n",
    "\n",
    "### Federated Learning\n",
    "- Training across **multiple decentralized clients**\n",
    "- Privacy-preserving (data stays local)\n",
    "- Challenges: fairness, convergence, communication\n",
    "\n",
    "SeedHash provides specialized metrics for all three!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ed8e",
   "metadata": {},
   "source": [
    "## 2. Semi-Supervised Learning üè∑Ô∏è\n",
    "\n",
    "Train with only 10% labeled data using label propagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a89dd5",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Semi-supervised learning experiment\n",
    "manager = SeedExperimentManager(\"semi_supervised_study\")\n",
    "\n",
    "# Generate seeds for SSL experiments\n",
    "hierarchy = manager.generate_seed_hierarchy(n_seeds=5, n_sub_seeds=2, max_depth=2)\n",
    "\n",
    "print(f\"Master seed: {hierarchy[0][0]}\")\n",
    "print(f\"Running {len(hierarchy[1])} SSL experiments\\n\")\n",
    "\n",
    "# Simulate semi-supervised learning\n",
    "for i, seed in enumerate(hierarchy[1][:3], 1):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Simulate metrics\n",
    "    n_samples = 100\n",
    "    n_labeled = int(n_samples * 0.10)  # Only 10% labeled!\n",
    "    n_unlabeled = n_samples - n_labeled\n",
    "    \n",
    "    labeled_acc = 0.85 + np.random.rand() * 0.10\n",
    "    pseudo_conf = 0.75 + np.random.rand() * 0.10\n",
    "    consistency = 0.80 + np.random.rand() * 0.10\n",
    "    \n",
    "    # Track SSL metrics\n",
    "    manager.add_experiment_result(\n",
    "        seed=seed,\n",
    "        ml_task=\"semi_supervised\",\n",
    "        metrics={\n",
    "            \"labeled_accuracy\": labeled_acc,\n",
    "            \"label_ratio\": (n_labeled / n_samples) * 100,\n",
    "            \"avg_pseudo_confidence\": pseudo_conf,\n",
    "            \"avg_consistency\": consistency,\n",
    "            \"n_labeled\": n_labeled,\n",
    "            \"n_unlabeled\": n_unlabeled\n",
    "        },\n",
    "        sampling_method=\"simple\",\n",
    "        metadata={\"method\": \"label_propagation\", \"iterations\": 10}\n",
    "    )\n",
    "    \n",
    "    print(f\"Experiment {i}:\")\n",
    "    print(f\"  Labeled accuracy: {labeled_acc:.3f}\")\n",
    "    print(f\"  Pseudo confidence: {pseudo_conf:.3f}\")\n",
    "    print(f\"  Only {n_labeled} labeled samples!\")\n",
    "\n",
    "print(\"\\n‚úì Semi-supervised experiments completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208a4ce",
   "metadata": {},
   "source": [
    "## 3. Semi-Supervised Metrics üìä\n",
    "\n",
    "SeedHash provides 10 specialized metrics for SSL:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `labeled_accuracy` | Accuracy on labeled data |\n",
    "| `label_ratio` | % of labeled vs total data |\n",
    "| `pseudo_label_diversity` | Variety in pseudo-labels |\n",
    "| `avg_pseudo_confidence` | Confidence in pseudo-labels |\n",
    "| `high_confidence_ratio` | % of high-confidence predictions |\n",
    "| `avg_consistency` | Agreement between predictions |\n",
    "| `consistency_std` | Stability of consistency |\n",
    "| `n_labeled` | Number of labeled samples |\n",
    "| `n_unlabeled` | Number of unlabeled samples |\n",
    "| `total_samples` | Total dataset size |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3441fd4",
   "metadata": {},
   "source": [
    "## 4. Reinforcement Learning üéÆ\n",
    "\n",
    "Train agents with episode-based learning (simulating CartPole environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450adabd",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Reinforcement learning experiment\n",
    "rl_manager = SeedExperimentManager(\"rl_cartpole_study\")\n",
    "\n",
    "# Generate seeds for RL training\n",
    "hierarchy = rl_manager.generate_seed_hierarchy(n_seeds=4, n_sub_seeds=2, max_depth=2)\n",
    "\n",
    "print(f\"Master seed: {hierarchy[0][0]}\")\n",
    "print(f\"Training {len(hierarchy[1])} RL agents\\n\")\n",
    "\n",
    "# Simulate RL training\n",
    "for i, seed in enumerate(hierarchy[1][:3], 1):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Simulate episode rewards (CartPole-like)\n",
    "    n_episodes = 100\n",
    "    rewards = np.random.exponential(120, n_episodes) + np.random.randn(n_episodes) * 10\n",
    "    episode_lengths = rewards + np.random.randn(n_episodes) * 5\n",
    "    success_threshold = 195\n",
    "    \n",
    "    mean_reward = np.mean(rewards)\n",
    "    success_rate = (rewards > success_threshold).mean() * 100\n",
    "    mean_episode_length = np.mean(episode_lengths)\n",
    "    \n",
    "    # Calculate improvement\n",
    "    early_rewards = rewards[:20].mean()\n",
    "    late_rewards = rewards[-20:].mean()\n",
    "    improvement = ((late_rewards - early_rewards) / early_rewards) * 100\n",
    "    \n",
    "    # Track RL metrics\n",
    "    rl_manager.add_experiment_result(\n",
    "        seed=seed,\n",
    "        ml_task=\"reinforcement\",\n",
    "        metrics={\n",
    "            \"mean_reward\": mean_reward,\n",
    "            \"std_reward\": np.std(rewards),\n",
    "            \"success_rate\": success_rate,\n",
    "            \"mean_episode_length\": mean_episode_length,\n",
    "            \"n_episodes\": n_episodes,\n",
    "            \"improvement_rate\": improvement\n",
    "        },\n",
    "        sampling_method=\"simple\",\n",
    "        metadata={\"environment\": \"CartPole-v1\", \"algorithm\": \"PPO\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Agent {i}:\")\n",
    "    print(f\"  Mean reward: {mean_reward:.1f}\")\n",
    "    print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"  Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\n‚úì RL training experiments completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f018fe1",
   "metadata": {},
   "source": [
    "## 5. RL Metrics & Tracking üìà\n",
    "\n",
    "SeedHash provides 12 specialized metrics for RL:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `mean_reward` | Average reward across episodes |\n",
    "| `std_reward` | Reward variability |\n",
    "| `max_reward` | Best episode performance |\n",
    "| `min_reward` | Worst episode performance |\n",
    "| `mean_episode_length` | Average episode duration |\n",
    "| `n_episodes` | Total episodes run |\n",
    "| `success_rate` | % of successful episodes |\n",
    "| `mean_q_value` | Average Q-value estimates |\n",
    "| `recent_mean_reward` | Recent performance trend |\n",
    "| `improvement_rate` | Learning progress |\n",
    "| `convergence_indicator` | Training stability |\n",
    "| `total_timesteps` | Total environment steps |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7334e",
   "metadata": {},
   "source": [
    "## 6. Federated Learning üåê\n",
    "\n",
    "Train across multiple clients while preserving privacy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5972a86",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Federated learning experiment\n",
    "fl_manager = SeedExperimentManager(\"federated_mnist_study\")\n",
    "\n",
    "# Generate seeds for FL rounds\n",
    "hierarchy = fl_manager.generate_seed_hierarchy(n_seeds=5, n_sub_seeds=2, max_depth=2)\n",
    "\n",
    "print(f\"Master seed: {hierarchy[0][0]}\")\n",
    "print(f\"Simulating {len(hierarchy[1])} FL rounds with 10 clients\\n\")\n",
    "\n",
    "# Simulate federated learning\n",
    "for i, seed in enumerate(hierarchy[1][:3], 1):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Simulate 10 clients\n",
    "    n_clients = 10\n",
    "    client_accuracies = 0.80 + np.random.rand(n_clients) * 0.15\n",
    "    global_accuracy = np.mean(client_accuracies)\n",
    "    \n",
    "    # Fairness: coefficient of variation\n",
    "    fairness_cv = np.std(client_accuracies) / np.mean(client_accuracies)\n",
    "    \n",
    "    # Model divergence (how different are client models?)\n",
    "    model_divergence = np.random.rand() * 0.3\n",
    "    \n",
    "    # Participation (% of clients that participated)\n",
    "    participation_rate = 0.80 + np.random.rand() * 0.19\n",
    "    \n",
    "    # Convergence indicator\n",
    "    convergence = 1.0 - (model_divergence / 0.3)\n",
    "    \n",
    "    # Track FL metrics\n",
    "    fl_manager.add_experiment_result(\n",
    "        seed=seed,\n",
    "        ml_task=\"federated\",\n",
    "        metrics={\n",
    "            \"global_accuracy\": global_accuracy,\n",
    "            \"accuracy_std\": np.std(client_accuracies),\n",
    "            \"accuracy_variance\": np.var(client_accuracies),\n",
    "            \"min_client_accuracy\": np.min(client_accuracies),\n",
    "            \"max_client_accuracy\": np.max(client_accuracies),\n",
    "            \"fairness_cv\": fairness_cv,\n",
    "            \"avg_model_divergence\": model_divergence,\n",
    "            \"avg_participation_rate\": participation_rate,\n",
    "            \"convergence_indicator\": convergence,\n",
    "            \"n_clients\": n_clients\n",
    "        },\n",
    "        sampling_method=\"simple\",\n",
    "        metadata={\"dataset\": \"MNIST\", \"federation_type\": \"IID\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"Round {i}:\")\n",
    "    print(f\"  Global accuracy: {global_accuracy:.3f}\")\n",
    "    print(f\"  Fairness (CV): {fairness_cv:.3f}\")\n",
    "    print(f\"  Client range: [{np.min(client_accuracies):.3f}, {np.max(client_accuracies):.3f}]\")\n",
    "\n",
    "print(\"\\n‚úì Federated learning rounds completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99288a",
   "metadata": {},
   "source": [
    "## 7. Federated Learning Metrics üåç\n",
    "\n",
    "SeedHash provides 14 specialized metrics for FL:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `global_accuracy` | Aggregated model accuracy |\n",
    "| `accuracy_std` | Std dev across clients |\n",
    "| `accuracy_variance` | Variance across clients |\n",
    "| `min_client_accuracy` | Worst performing client |\n",
    "| `max_client_accuracy` | Best performing client |\n",
    "| `fairness_cv` | Coefficient of variation (fairness) |\n",
    "| `global_loss` | Aggregated loss |\n",
    "| `loss_std` | Loss variability |\n",
    "| `avg_model_divergence` | Client model differences |\n",
    "| `max_model_divergence` | Largest divergence |\n",
    "| `avg_participation_rate` | % of active clients |\n",
    "| `convergence_indicator` | Training stability |\n",
    "| `communication_rounds` | Total FL rounds |\n",
    "| `n_clients` | Number of clients |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44895e",
   "metadata": {},
   "source": [
    "## 8. Complete Example: Multi-Paradigm Study üî¨\n",
    "\n",
    "Compare all three advanced paradigms in one comprehensive study!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe0d47",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Combine all three paradigms in one study\n",
    "complete_study = SeedExperimentManager(\"multi_paradigm_comparison\")\n",
    "\n",
    "# Analyze all experiments\n",
    "all_results = []\n",
    "\n",
    "# Collect SSL results\n",
    "for result in manager.results:\n",
    "    all_results.append(result)\n",
    "\n",
    "# Collect RL results\n",
    "for result in rl_manager.results:\n",
    "    all_results.append(result)\n",
    "\n",
    "# Collect FL results\n",
    "for result in fl_manager.results:\n",
    "    all_results.append(result)\n",
    "\n",
    "# Create unified DataFrame\n",
    "complete_study.results = all_results\n",
    "df = complete_study.get_results_dataframe()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-PARADIGM STUDY RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTotal experiments: {len(df)}\")\n",
    "print(f\"\\nExperiments by paradigm:\")\n",
    "print(df['ml_task'].value_counts())\n",
    "\n",
    "print(f\"\\n\\nKey metrics by paradigm:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# SSL metrics\n",
    "ssl_df = df[df['ml_task'] == 'semi_supervised']\n",
    "if len(ssl_df) > 0:\n",
    "    print(f\"\\nSemi-Supervised Learning ({len(ssl_df)} experiments):\")\n",
    "    print(f\"  Avg labeled accuracy: {ssl_df['metric_labeled_accuracy'].mean():.3f}\")\n",
    "    print(f\"  Avg pseudo confidence: {ssl_df['metric_avg_pseudo_confidence'].mean():.3f}\")\n",
    "\n",
    "# RL metrics\n",
    "rl_df = df[df['ml_task'] == 'reinforcement']\n",
    "if len(rl_df) > 0:\n",
    "    print(f\"\\nReinforcement Learning ({len(rl_df)} experiments):\")\n",
    "    print(f\"  Avg reward: {rl_df['metric_mean_reward'].mean():.1f}\")\n",
    "    print(f\"  Avg success rate: {rl_df['metric_success_rate'].mean():.1f}%\")\n",
    "\n",
    "# FL metrics\n",
    "fl_df = df[df['ml_task'] == 'federated']\n",
    "if len(fl_df) > 0:\n",
    "    print(f\"\\nFederated Learning ({len(fl_df)} experiments):\")\n",
    "    print(f\"  Avg global accuracy: {fl_df['metric_global_accuracy'].mean():.3f}\")\n",
    "    print(f\"  Avg fairness (CV): {fl_df['metric_fairness_cv'].mean():.3f}\")\n",
    "\n",
    "# Export combined results\n",
    "df.to_csv('all_advanced_paradigms.csv', index=False)\n",
    "print(f\"\\n‚úì Exported {len(df)} experiments to 'all_advanced_paradigms.csv'\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e12747",
   "metadata": {},
   "source": [
    "## 9. Best Practices üí°\n",
    "\n",
    "### Semi-Supervised Learning\n",
    "‚úÖ **Do:**\n",
    "- Use when labeled data is expensive\n",
    "- Monitor pseudo-label confidence\n",
    "- Track consistency across iterations\n",
    "- Start with high-confidence threshold\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Use with extremely small labeled sets (<1%)\n",
    "- Ignore label quality metrics\n",
    "- Skip consistency checks\n",
    "\n",
    "### Reinforcement Learning\n",
    "‚úÖ **Do:**\n",
    "- Track episode rewards over time\n",
    "- Monitor success rates\n",
    "- Calculate improvement rates\n",
    "- Use multiple seeds for robustness\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Evaluate on too few episodes\n",
    "- Ignore reward variance\n",
    "- Skip convergence checks\n",
    "\n",
    "### Federated Learning\n",
    "‚úÖ **Do:**\n",
    "- Monitor client fairness\n",
    "- Track model divergence\n",
    "- Check participation rates\n",
    "- Measure convergence\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Ignore stragglers (slow clients)\n",
    "- Skip fairness metrics\n",
    "- Overlook privacy guarantees\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939288fd",
   "metadata": {},
   "source": [
    "## 10. Summary üéâ\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "‚úÖ **Semi-Supervised Learning**\n",
    "- Training with minimal labeled data\n",
    "- 10 specialized SSL metrics\n",
    "- Pseudo-labeling and consistency tracking\n",
    "\n",
    "‚úÖ **Reinforcement Learning**\n",
    "- Episode-based agent training\n",
    "- 12 specialized RL metrics\n",
    "- Reward tracking and convergence\n",
    "\n",
    "‚úÖ **Federated Learning**\n",
    "- Privacy-preserving distributed training\n",
    "- 14 specialized FL metrics\n",
    "- Fairness and convergence monitoring\n",
    "\n",
    "### Total Capabilities:\n",
    "- **7 ML paradigms** supported\n",
    "- **36+ specialized metrics** across all paradigms\n",
    "- **4 sampling methods** for experiments\n",
    "- **Hierarchical seeds** for nested experiments\n",
    "- **DataFrame export** for analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps üöÄ\n",
    "\n",
    "1. Apply these techniques to your own data\n",
    "2. Experiment with different sampling methods\n",
    "3. Track metrics over multiple runs\n",
    "4. Export results for publication\n",
    "5. Contribute to the seedhash project!\n",
    "\n",
    "**GitHub**: https://github.com/melhzy/seedhash\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing all 3 tutorials! üéä**\n",
    "\n",
    "You now have expert-level knowledge of reproducible ML experimentation with SeedHash!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
